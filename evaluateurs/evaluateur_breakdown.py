# -*- coding: utf-8 -*-
"""Evaluateur_BreakDown.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bU7PzQO8_GxsHr_D6o6GMxirEelwyfPt
"""

import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from pyBreakDown.explainer import Explainer
from pyBreakDown.explanation import Explanation
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor
from tensorflow import keras 
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
#from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf
#import tensorflow_datasets as tfds
from keras import optimizers
import warnings
warnings.filterwarnings("ignore")


#build NN model
def create_model():
# create model
  model = Sequential()
  model.add(Dense(50, activation='relu'))
  model.add(Dense(100, activation='relu'))
  model.add(Dense(50, activation='relu'))
  model.add(Dense(1))
 #model.compile(loss="categorical_crossentropy", optimizer='keras.optimizers.RMSprop()', metrics=["accuracy"])
  model.compile(loss='mean_squared_error', optimizer='adam')
  return model

#----------------------------------------------------------------------------------------------------------------#


def get_modele(modele, train_data, train_labels):
    # Modele
    
    if (modele == "Arbre"):
        model = DecisionTreeClassifier().fit(train_data,y=train_labels)
    elif (modele == "Régression Logistique"):
        model = LogisticRegression().fit(train_data, train_labels)
    elif (modele == "KNN"):
        model = KNeighborsClassifier(n_neighbors=5).fit(train_data, train_labels)
    elif (modele == "Random Forest"):
        model = RandomForestClassifier().fit(train_data, train_labels)
    elif (modele == "SVM"):
        model = SVC().fit(train_data, train_labels)
    #elif (modele == "NN"):
        #target = 'medv'
        #model = KerasClassifier(build_fn=create_model, epochs=10)
        #model.fit(train_data, train_labels)
    else: 
        return("Ce nom de modèle n'est pas pris en charge")
    return model

def eval_bd(data, y, feature_names, nb_tw, modele):

    np.set_printoptions(suppress=True) #Supprime la notation scientifique qui pose problème à l'explicateur
    
    train_data, test_data, train_labels, test_labels = train_test_split(data, y, test_size=0.3, random_state=1)

    model = get_modele(modele, train_data, train_labels)
    
    #Construction de l'explainer de pyBreakDown
    exp = Explainer(clf=model, data=train_data, colnames=feature_names)
    nb_diff = 0 
    n_test = len(test_labels) #Nombre d'invidividus dans l'échantillon test
    for i in range (0, n_test) : #Pour chaque individu dans l'échantillon test
        #On fait son explication BreakDown
        explanation = exp.explain(observation=test_data[i,:],direction="up")
        #On prédit la valeur de l'individu
        pred_before = model.predict([data[i,:]])

        #------------------On trie les variables par significativité pour cette prédiction
        attributes1 = pd.DataFrame(explanation._attributes)
        attributes1 = attributes1[attributes1[0] != "Intercept"].reset_index(drop=True)
        attributes2 = attributes1[2].copy()
        attributes2 = abs(np.array(attributes2))
        attributes2 = pd.DataFrame(attributes2, columns=['abs'])
        attributes = pd.concat([attributes1, attributes2], axis=1)
        #attributes = pd.DataFrame(attributes, columns=['name', 'value', 'contribution', 'cumulative', 'contrib_abs'])
        attributes = attributes.sort_values(by='abs', ascending=False)
        #-------------------

        #utw = untrustworthy
        #n : nombre de variables explicatives
        n = len(attributes)

        #utw stock les variables untrustworthy pour cette prédiction et tw les trustworthy
        utw = attributes.iloc[nb_tw:n, :]
        utw_names = utw.iloc[:,0]
        #tw contient les nb_tw variables les plus significatives et utw le reste
        tw = attributes.iloc[0:nb_tw, :]
        tw_names = tw.iloc[:,0]

        data2 = np.copy(data)
        data2 = pd.DataFrame(data2, columns=feature_names)
        data2 = np.array(data2.drop(columns=utw_names))

        train_data2, test_data2, train_labels, test_labels = train_test_split(data2, y, test_size=0.3, random_state=1)
        model2 = get_modele(modele, train_data2, train_labels)
        pred_after = model2.predict([data2[i,:]])


        if ((pred_after-pred_before) != 0):
          nb_diff = nb_diff + 1
    #print("Bonnes explicabilités : {} sur {}".format(n_test-nb_diff, n_test))
    return(nb_diff/n_test)
